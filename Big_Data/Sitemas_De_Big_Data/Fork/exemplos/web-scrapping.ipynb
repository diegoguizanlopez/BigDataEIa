{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar librarías con conda:\n",
    "\n",
    "  conda install requests beautifulsoup4\n",
    "\n",
    "- Distinguir URL base de URL con parámetros\n",
    "- Como funcionan os parámetros? (web dinámica vs estática)\n",
    "- Vendo o HTML e a árbore DOM con Developer tools. Diferenciar tags con class e id."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visitar a web de exemplo\n",
    "\n",
    "Imos empregar a web de exemplo: <https://realpython.github.io/fake-jobs/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo unha URL con requests\n",
    "\n",
    "Este é o modo máis simple de descargar unha web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "# Dentro de .text teremos o código da páxina\n",
    "page.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engadindo o parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Para poder parsear HTML\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atopar elementos por ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find(id=\"ResultsContainer\")\n",
    "\n",
    "print(results.prettify())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atopar elementos por class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elements = results.find_all(\"div\", class_=\"card-content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterando polos elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in job_elements:\n",
    "    print(job_element.prettify(), end=\"\\n\"*2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O anterior xera demasiado HTML, mellor collemos tan so partes: Traballo, compañía e ubicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in job_elements:\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_element.find(\"p\", class_=\"location\")\n",
    "    print(title_element)\n",
    "    print(company_element)\n",
    "    print(location_element)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo en conda que cada job_element é outro obxecto tipo BeautifulSoup, podemos quitar o HTML molesto\n",
    "\n",
    "Tamén metemos o método .strip() para quitar espacios ao principio e final: <https://www.w3schools.com/python/ref_string_strip.asp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in job_elements:\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_element.find(\"p\", class_=\"location\")\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Tamén podemos buscar os elementos por clase que conteñen algún texto****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all(\"h2\", string=\"Senior Python Developer\")\n",
    "for i in python_jobs:\n",
    "    print(i)\n",
    "\n",
    "print (\"---\")\n",
    "python_jobs = results.find_all(\"h2\", string=\"Python\")\n",
    "for i in python_jobs:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non amosa resultados porque busca un texto que sexa exactamente igual. Espazos en branco, letras maiúsculas ou minúsculas, guións e outras variacións farán que non se atopen os resultados como queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all(\n",
    "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imprimimos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (python_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mellor elemento a elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in python_jobs:\n",
    "    print (job)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E aínda mellor se quitamos o HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in python_jobs:\n",
    "    print (job.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E se xuntamos todo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "python_jobs = results.find_all(\n",
    "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
    ")\n",
    "\n",
    "for job_element in python_jobs:\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_element.find(\"p\", class_=\"location\")\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vaia! Fallou! Por que?\n",
    "**Pista**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in python_jobs:\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    print (title_element)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hai elementos h2 coa clase title que inclúan a información que buscamos?\n",
    "\n",
    "So temos o nome do traballo. De ahí que non temos nada mais: \n",
    "\n",
    "    <h2 class=\"title is-5\">Senior Python Developer</h2>\n",
    "\n",
    "Teríamos que acceder ao pai e de ahí sacar un obxecto que nos permita acceder ás súas propiedades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all(\n",
    "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
    ")\n",
    "\n",
    "python_job_elements = [\n",
    "    h2_element.parent.parent.parent for h2_element in python_jobs\n",
    "]\n",
    "\n",
    "for job_element in python_job_elements:\n",
    "    links = job_element.find_all(\"a\")\n",
    "    for link in links:\n",
    "        link_url = link[\"href\"]\n",
    "        print(f\"Apply here: {link_url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duas ligazóns? Non hai problema se queremos só a segunda..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in python_job_elements:\n",
    "    link_url = job_element.find_all(\"a\")[1][\"href\"]\n",
    "    print(f\"Apply here: {link_url}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercicio**: Meter na BD MySQL os resultados nunha táboa.\n",
    "\n",
    "    CREATE TABLE fakejob(\n",
    "\t    position VARCHAR(200),\n",
    "\t    company VARCHAR(200),\n",
    "        address VARCHAR(200),\n",
    "\t    pubDate VARCHAR(200),\n",
    "\t    url VARCHAR(250)\n",
    "    );\n",
    "\n",
    "Scrapping de dous ditios mais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "db_host = \"localhost\"\n",
    "db_port=3306\n",
    "db_user = \"usuario\"\n",
    "db_passwd=\"abc123.\"\n",
    "db_name=\"employees\"\n",
    "\n",
    "#Xerar a cadea de conexión en base aos parámetros anteriores\n",
    "connectionString=f'mysql+pymysql://{db_user}:{db_passwd}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "engine = create_engine(connectionString)\n",
    "\n",
    "job_elements = results.find_all(\"div\", class_=\"card-content\")\n",
    "\n",
    "for job_element in job_elements:\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_element.find(\"p\", class_=\"location\")\n",
    "    publication_date = job_element.find(\"time\")\n",
    "    link_url = job_element.find_all(\"a\")[1][\"href\"]\n",
    "    cadeaSQL = f'''INSERT INTO fakejob(position, company, address, pubDate, url) VALUES(\n",
    "                    '{title_element.text.strip()}',\n",
    "                    '{company_element.text.strip()}', \n",
    "                    '{location_element.text.strip()}', \n",
    "                    '{publication_date.text.strip()}', \n",
    "                    '{link_url}')'''\n",
    "    result=engine.execute(cadeaSQL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca o contido da cabeceira H1\n",
    "soup.h1.text\n",
    "\n",
    "# Mostra a ruta á imaxe que se mostra na web\n",
    "soup.img.get('src')\n",
    "\n",
    "# Mostra o texto alternativo da imaxe\n",
    "soup.img.get('alt')\n",
    "\n",
    "# Mostra todos os textos \"strongly\"-resaltados da páxina\n",
    "soup.find_all('strong')\n",
    "\n",
    "# Mostra todos os enlaces presentes na páxina\n",
    "for i in soup.find_all('a'):\n",
    "    print(i.get('href'))\n",
    "\n",
    "# Mostra os textos/palabras que teñen enlace \n",
    "# Mostra todos os enlaces presentes na páxina\n",
    "for i in soup.find_all('a'):\n",
    "    print(i.text)\n",
    "\n",
    "# Conta o número de parágrafos presentes na páxina web\n",
    "contador = 0\n",
    "for i in soup.find_all('p'):\n",
    "    contador = contador + 1\n",
    "contador\n",
    "# ou directamente utilizar len\n",
    "# len(soup.find_all('p'))\n",
    "\n",
    "# Mostra o contido do último parágrafo\n",
    "soup.find_all('p')[-1].text\n",
    "\n",
    " \n",
    "\n",
    "# NOVA PÁXINA: https://bigdatawirtz.github.io/exemplo-web/08.html\n",
    "# Bótalle unha ollada ao código da páxina\n",
    "url = 'https://bigdatawirtz.github.io/exemplo-web/08.html'\n",
    "paxina = requests.get(url)\n",
    "\n",
    "print(paxina.text)\n",
    "\n",
    "# Parsear o contido da web\n",
    "soup = BeautifulSoup(paxina.content, 'html.parser')\n",
    "\n",
    "# Mostra o título da páxina\n",
    "soup.title.text\n",
    "\n",
    "# Mostra o charset da páxina, dentro de \"meta\"\n",
    "soup.meta.get('charset')\n",
    "\n",
    "# Conta o número de parágrafos que ten a páxina\n",
    "len(soup.find_all('p'))\n",
    "\n",
    "# Mostrar o texto no pé do artigo\n",
    "soup.footer.p.text\n",
    "\n",
    "# Mostrar o texto no pé da web\n",
    "soup.find_all('footer')[-1].p.text\n",
    "\n",
    "# Mostrar id da sección\n",
    "soup.section.get('id')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fonte** (adaptado de): https://realpython.com/beautiful-soup-web-scraper-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f2c8ca1a79d6d82f32e116612cbd8f0319de1d4a0a5509748c0bfa14525846a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
