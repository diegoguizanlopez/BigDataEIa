{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNgB-24L4DoV"
   },
   "source": [
    "**Créditos**: baseado nun código visto en: https://medium.com/@a.fernandez.troyano/nube-de-palabras-word-cloud-con-python-a-partir-de-varias-webs-111e94220822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repo orixinal: https://github.com/bigdatawirtz/nubes/blob/main/nube_de_respostas.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facendo unha nube de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando dependencias\n",
    "\n",
    "- conda activate bigdata\n",
    "\n",
    "- conda install numpy pandas\n",
    "\n",
    "- conda install -c conda-forge wordcloud nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYHUlxpe3C8N"
   },
   "source": [
    "## Cargamos librarías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dV5xxIb_xPjM"
   },
   "outputs": [],
   "source": [
    "#Librerías básicas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Librerías para imaxes, nubes de palabras e plot\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Librarías para limpieza de datos\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBDz75RT0_NX"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8gnQoRJ3Kz9"
   },
   "source": [
    "**Carga de texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kO-EWcqtyanY"
   },
   "outputs": [],
   "source": [
    "texto = \"\"\"\n",
    "El Big Data es una cosa muy buena. Muy buena tiene que ser, y muy grande, por lo de Big.\n",
    "El Big Data es cuando tratas muchos datos.\n",
    "El Big Data es utilizar muchos datos para extraer información.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLm5R967zHs2"
   },
   "outputs": [],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn-LBb_z3X_5"
   },
   "source": [
    "**Limpeza do texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVDLW9HD0kcX"
   },
   "outputs": [],
   "source": [
    "#Generación de lista de signos de puntuación\n",
    "\n",
    "punctuation=[]\n",
    "for s in string.punctuation:\n",
    "    punctuation.append(str(s))\n",
    "sp_punctuation = [\"¿\", \"¡\", \"“\", \"”\", \"…\", \":\", \"–\", \"»\", \"«\"]    \n",
    "\n",
    "punctuation += sp_punctuation\n",
    "\n",
    "punctuation[:10] #Ejemplo de los símbolos de puntuación que están incluidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDYp8z770sxj"
   },
   "outputs": [],
   "source": [
    "#Listado de palabras que queremos eliminar del texto\n",
    "#Es un proceso iterativo por lo que si después vemos que nos siguen quedado \"caractéres raros\" simplemente venímos aquí y los agregamos\n",
    "#Existe librerías y listados de \"Stop_words\", pero por ahora vamos a dejarlo vacío\n",
    "\n",
    "#nltk.download('stopwords') #La primera vez debemos descargar las \"stopwords\"\n",
    "\n",
    "stop_words = stopwords.words('spanish') #Listado de palabras a eliminar\n",
    "\n",
    "stop_words += [\"\\u200b\", \"\\xa0\", \"para\", \"como\", \"puede\",\"cómo\", \"hacer\", \"forma\", \"parte\", \"hace\", \"además\", \"según\", \"pueden\", \"ser\"] #Añadimos algunos caractéres que hemos encontrado\n",
    "\n",
    "stop_words[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2W7n4WF1Jew"
   },
   "outputs": [],
   "source": [
    "#Reemplazamos signos de puntuación por \"\":\n",
    "for sw in stop_words:\n",
    "    clean_texto = texto.lower().replace(sw,\"\")\n",
    "    \n",
    "for p in punctuation:\n",
    "    clean_texto = clean_texto.replace(p,\"\")\n",
    "\n",
    "#Eliminamos espacios blancos, saltos de línea, tabuladores, etc    \n",
    "#clean_texto = \" \".join(clean_texto.split())    \n",
    "\n",
    "#Reemplazamos stop_words por \"\":    \n",
    "for stop in stop_words:\n",
    "    clean_texto_list = clean_texto.split()\n",
    "    clean_texto_list = [i.strip() for i in clean_texto_list]\n",
    "    try:\n",
    "        while stop in clean_texto_list: clean_texto_list.remove(stop)\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        pass\n",
    "    clean_texto= \" \".join(clean_texto_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UBUs-Mz0wTm"
   },
   "outputs": [],
   "source": [
    "lista_texto = clean_texto.split(\" \")\n",
    "\n",
    "palabras = []\n",
    "\n",
    "#Paso intermedio para eliminar palabras muy cortas (emoticonos,etc) y muy largas (ulr o similar) que se nos hayan pasado:\n",
    "\n",
    "for palabra in lista_texto:\n",
    "    if (len(palabra)>=3 and len(palabra)<18):\n",
    "        palabras.append(palabra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCFWApD23pt_"
   },
   "source": [
    "**Contaxe de palabras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IAeRVsp1R0K"
   },
   "outputs": [],
   "source": [
    "#Generamos un diccionario para contabilizar las palabras:\n",
    "\n",
    "word_count={}\n",
    "\n",
    "for palabra in palabras:\n",
    "    if palabra in word_count.keys():\n",
    "        word_count[palabra][0]+=1\n",
    "    else:\n",
    "        word_count[palabra]=[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYE-U4m51T1M"
   },
   "outputs": [],
   "source": [
    "#Generamos el DF y lo ordenamos:\n",
    "\n",
    "df = pd.DataFrame.from_dict(word_count).transpose()\n",
    "df.columns=[\"freq\"]\n",
    "df.sort_values([\"freq\"], ascending=False, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-1YOQex3wdU"
   },
   "source": [
    "**Mostrar gráfica de ocorrencias das palabras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IZc7CUk1V8n"
   },
   "outputs": [],
   "source": [
    "def plot_bar(data=df, top=5):    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,2,1])\n",
    "    ax.bar(x =df.iloc[:top,:].index, height = df.iloc[:top,0].values)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykL_y-oJ32Nx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7n_tViiV1Y1O"
   },
   "outputs": [],
   "source": [
    "#Graficamos el TOP 5 palabras por frecuencia\n",
    "\n",
    "plot_bar(data=df, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DipDVZS341f"
   },
   "source": [
    "**Crear nube de palabras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2E38xVL1bUP"
   },
   "outputs": [],
   "source": [
    "#word_cloud = WordCloud(height=800, width=800, background_color='white',max_words=150, min_font_size=5, collocation_threshold=10).generate(clean_texto)\n",
    "word_cloud = WordCloud(height=800, width=800, background_color='white',max_words=150, min_font_size=5).generate(clean_texto)\n",
    "\n",
    "#word_cloud.to_file(\"./img/ejemplo_sencillo.png\") #Guardamos la imagen generada\n",
    "#word_cloud.to_file(\"./img/ejemplo_sencillo.png\") #Guardamos la imagen generada\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Izhn-N031e3n"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOiatDXQqEOthlfcaxzUXa1",
   "include_colab_link": true,
   "name": "nube_de_respostas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "09bd56ac4f16ae3235245e0149ff29ae6c8fe56d9059f2dcfde90f0a487d15cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
