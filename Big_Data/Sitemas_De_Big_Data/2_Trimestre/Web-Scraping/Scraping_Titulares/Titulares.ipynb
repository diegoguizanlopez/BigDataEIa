{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scraping de titulares de varios periódicos**\n",
    "\n",
    "## 1.Scrapeo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(uri):\n",
    "    \"\"\"Devuelve el request sobre una URL\n",
    "\n",
    "    Args:\n",
    "        uri (str): La url de donde obtener el Request\n",
    "\n",
    "    Returns:\n",
    "        Request: La request a la URL\n",
    "    \"\"\"\n",
    "    ua = \"Mozilla/5.0 (Linux; U; Android 2.2; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\"  \n",
    "    h = {\"User-Agent\": ua}\n",
    "    httpPool = urllib3.PoolManager()\n",
    "    return httpPool.request('GET',uri,fields=None,headers=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claves = {}\n",
    "\n",
    "def get_data(map):\n",
    "    \"\"\"Devuelve la Lista de Datos transformados para su utilización\n",
    "\n",
    "    Args:\n",
    "        Map (Map<Key,Value>): Mapa con el periódico y los valores\n",
    "\n",
    "    Returns:\n",
    "        List: Frase Str, Url Str, Peso Float\n",
    "    \"\"\"\n",
    "    dato=map[1]\n",
    "    datos=[]\n",
    "    soup = BeautifulSoup(get_request(dato[1]).data,'html.parser')\n",
    "    urls=[]\n",
    "    for text in soup.find_all(dato[2],{\"class\":dato[3]} if len(dato[3])>0 else None):\n",
    "        texto = text.text.replace(\"\\n\",\"\")\n",
    "        texto = texto.replace(\"\\r\",\"\")\n",
    "        texto = texto.replace(\"\\t\",\"\")\n",
    "        if dato[4] == \".\":\n",
    "            enlace = text\n",
    "            bEnlace = text[dato[5]]\n",
    "        else:\n",
    "            enlace=text.findChildren(dato[4] , recursive=False)\n",
    "            if len(enlace)==0:\n",
    "                continue\n",
    "            bEnlace = enlace[0][dato[5]]\n",
    "        if bEnlace.startswith(\"/\"):\n",
    "            bEnlace=dato[1]+bEnlace\n",
    "        datosenlace=[texto,bEnlace,0]\n",
    "        datos.append(datosenlace)\n",
    "        urls.append(bEnlace)\n",
    "    claves[dato[0]]=urls\n",
    "    return datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Recogemos los datos del fichero .json \n",
    "with open('Media/medios.json',\"r\") as f:\n",
    "    urls = json.load(f)\n",
    "\n",
    "\n",
    "DATA=[]\n",
    "# Guardamos los valores transformados en una lista con su key para trastear con ellos\n",
    "for key,x in urls.items():\n",
    "    DATA.append([key,x['url'],x['tag'],x['clase'],x['tagURL'],x['valueURL']])\n",
    "\n",
    "# Creamos un ThreadPool donde se ejecutará en varios procesos la búsqueda de datos, sin esto en búsqueda de 10 páginas tardaría más de 10 segundos\n",
    "pool = ThreadPool(len(DATA))\n",
    "datos=[]\n",
    "v = pool.map(get_data, enumerate(DATA)) \n",
    "datos = [item for lista in v for item in lista]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos aquí los datos de cada página con toda su URL\n",
    "\n",
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('spanish')\n",
    "\n",
    "stop_words += [\"\\u200b\", \"\\xa0\", \"para\", \"como\", \"puede\",\"cómo\", \"hacer\", \"forma\", \"parte\", \"hace\", \"además\", \"según\", \"pueden\", \"ser\",\"tras\"]\n",
    "\n",
    "stop_words[:10]\n",
    "\n",
    "# Definimos los StopWords para eliminar en el contador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos a un regex todas las palabras no permitidas\n",
    "regex = \"|\".join(stop_words)\n",
    "\n",
    "# Hacemos deepcopy de los datos que trabajamos luego en el conteo\n",
    "frasesEditadas = deepcopy(datos) \n",
    "for index,dato in enumerate(datos):\n",
    "    viejaFrase=deepcopy(dato[0])\n",
    "    # Filtramos las palabras\n",
    "    fraseNueva = re.sub(f\"\\\\b({regex})\\\\b\", \" \", viejaFrase)\n",
    "    frasecompleta = fraseNueva.strip ()\n",
    "    palabras = frasecompleta.split ()\n",
    "    # Juntamos la frase completa con filtros que eliminan caracteres no deseados\n",
    "    frasecompleta = \" \".join([re.sub(r'^\\W|\\W$', '', p.lower()) for p in palabras if len(re.sub(r'^\\W|\\W$', '', p)) > 3])\n",
    "    frasesEditadas[index] = [frasecompleta,frasesEditadas[index][1],frasesEditadas[index][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos aquí las frases editadas\n",
    "\n",
    "frasesEditadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias = {}\n",
    "\n",
    "# Recorro en el conteo de frecuencia de palabras\n",
    "textoEntero = \" \".join([sublista[0] for sublista in frasesEditadas])\n",
    "for palabra in textoEntero.split():\n",
    "    palabra = palabra.strip() \n",
    "    if palabra in frecuencias:\n",
    "        frecuencias[palabra] += 1 # Aumenta la frecuencia si la palabra ya existe\n",
    "    else:\n",
    "        frecuencias[palabra] = 1 # Crea una nueva entrada si la palabra no existe\n",
    "\n",
    "# Y calculo el valor de cada frase editada \n",
    "for index,portadaE in enumerate(frasesEditadas):\n",
    "    palabras = portadaE[0].split()\n",
    "    valor=0\n",
    "    for palabra in palabras:\n",
    "      valor+=frecuencias[palabra]\n",
    "    l = len(datos[index][0].split())\n",
    "    frasesEditadas[index][2]=valor/(l)\n",
    "    datos[index][2]=valor/l\n",
    "\n",
    "frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saco el valor relativo de cada una de las frases\n",
    "\n",
    "tope=max(datos,key= lambda x:x[2])[2]\n",
    "for index,i in enumerate(datos):\n",
    "    datos[index][2]=(i[2]/tope)\n",
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titularesOrdenados=sorted(datos,key= lambda x:x[2],reverse=True)\n",
    "\n",
    "# Borramos si coincide titular\n",
    "dicc = {}\n",
    "for x in titularesOrdenados:\n",
    "    dicc[x[0]] = x\n",
    "\n",
    "titularesOrdenados = list(dicc.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Amosar un gráfico de barras coas 10 palabras que se repiten máis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ordenamos el listado y pillamos los mejores 10\n",
    "diccionario_ordenado = sorted(frecuencias.items(), key=lambda x: x[1], reverse=True)\n",
    "diez_mas_grandes = dict(diccionario_ordenado[:10])\n",
    "\n",
    "# Los ponemos en el plt.bar\n",
    "plt.bar(diez_mas_grandes.keys(),diez_mas_grandes.values())\n",
    "plt.xlabel(\"PALABRAS\")\n",
    "plt.xticks(rotation=-90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Escribir a un arquivo HTML: resultado-ano-mes-dia.html (ordeado polo peso, de modo que os titulares que aparecen máis grandes, aparezan ao inicio)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librería para manejo de HTML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y -c conda-forge airium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from airium import Airium\n",
    "\n",
    "a = Airium()\n",
    "\n",
    "\n",
    "# Librería para el manejo de HTML\n",
    "a('<!DOCTYPE html>')\n",
    "with a.html(lang=\"pl\"):\n",
    "    with a.head():\n",
    "        a.meta(charset=\"utf-8\")\n",
    "        a.title(_t=\"Página de noticias\")\n",
    "    with a.body():\n",
    "        # Decimos que por cada titular ordenado ponemos el peso relativo +1, la URL a la noticia como enlace y el texto de la noticia\n",
    "        for portada in titularesOrdenados:\n",
    "            with a.p(style=f\"font-size:{portada[2]+1}em;\"):\n",
    "                with a.a(href=portada[1]):\n",
    "                    a(portada[0])\n",
    "\n",
    "html = str(a)  # Casteo de los datos\n",
    "\n",
    "# Escribimos el HTML con el año, mes y día\n",
    "with open(f\"HTML/resultado-{time.localtime().tm_year}-{time.localtime().tm_mon}-{time.localtime().tm_mday}.html\",\"w+\") as f:\n",
    "    f.write(html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. **Amosar por pantalla os 20 titulares e a URL que teñan maior peso relativo (“relevancia”)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorremos los titulares ordenados anteriormente y recogemos los 20 primeros\n",
    "for i in titularesOrdenados[:20]:\n",
    "    print(f\"URL: {i[1]} - FRASE: {i[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. **Suxire un método mellor para analizar a relevancia dos titulares**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede utilizar el análisis de de sentimiento, esto implica utilizar técnicas de procesamiento de lenguaje natural para determinar si el titular es positivo, natural o neutro, esto implica que los positivos y negativos seán más relevantes, mientras que los neutros los sean menos, teniendo en cuenta también la repetición de palabras y su peso en las frases, por lo que nos daría una mayor precisión juntando los dos métodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. **Mete os titulares, medio, URL e peso relativo nunha BBDD de SQLite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y sqlite3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Conectamos siguiendo el patrón SingleTone\n",
    "if not(connection):\n",
    "    connection=sqlite3.connect(\"Media/titulares.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el cursor y si existe la tabla en la BBDD nos lo dice\n",
    "cursor = connection.cursor()\n",
    "try:\n",
    "    cursor.execute(\"CREATE TABLE titulares (titular TEXT,medio TEXT, url TEXT, prelativo REAL)\")\n",
    "except:\n",
    "    print(\"La tabla ya está creada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscador_titular_peso(URL):\n",
    "    \"\"\"Devuelve el titular y su peso relativo\n",
    "\n",
    "    Args:\n",
    "        URL (str): URL del titular\n",
    "\n",
    "    Returns:\n",
    "        List: Devuelve el titular y peso\n",
    "    \"\"\"\n",
    "    for i in datos:\n",
    "        if i[1] == URL:\n",
    "            return i[0],i[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = []\n",
    "\n",
    "# Buscamos los datos a insertar y lo ejecutamos con executemany\n",
    "for key,values in claves.items():\n",
    "    for url in values:\n",
    "        titular,peso=buscador_titular_peso(url)\n",
    "        valores.append([titular,key,url,peso])\n",
    "\n",
    "\n",
    "cursor.executemany(\"\"\"\n",
    "    INSERT INTO titulares ('titular', 'medio', 'url', 'prelativo')\n",
    "    VALUES (?,?,?,?)\"\"\", valores)\n",
    "\n",
    "# Si quisieramos insertar y que los cambios no queden pendientes\n",
    "#connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que se insertan o por lo menos están para commit\n",
    "\n",
    "cursor.execute(\"SELECT * FROM titulares order by prelativo desc\")\n",
    "\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y cerramos conexión\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
