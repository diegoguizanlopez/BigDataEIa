{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q gensim\n",
    "#!pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "path=str(pathlib.Path().resolve())+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\diego.guizanlopez\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"The Titanic sailed with hope and illusion to cross the Atlantic.\"\n",
    "text2 = \"The sinking of the Titanic was one of the greatest maritime tragedies.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "token1 = word_tokenize(text1.lower())\n",
    "token2 = word_tokenize(text2.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos al tokenizador para que aprenda a separar frases, palabras\n",
    "# Ponemos el tama침o del Vector 10 para probar\n",
    "# Tama침o de salto de palabras es decir, palabras que contara seguidas\n",
    "# Min count para que tenga que aparecer X veces para que haga caso\n",
    "\n",
    "model = Word2Vec([token1,token2],vector_size=10,window=5,min_count=1,sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'titanic',\n",
       " 'sailed',\n",
       " 'with',\n",
       " 'hope',\n",
       " 'and',\n",
       " 'illusion',\n",
       " 'to',\n",
       " 'cross',\n",
       " 'the',\n",
       " 'atlantic',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'sinking',\n",
       " 'of',\n",
       " 'the',\n",
       " 'titanic',\n",
       " 'was',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'maritime',\n",
       " 'tragedies',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07380505, -0.01533471, -0.04536613,  0.06554051, -0.0486016 ,\n",
       "       -0.01816018,  0.0287658 ,  0.00991874, -0.08285215, -0.09448818],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['titanic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'titanic',\n",
       " 'of',\n",
       " '.',\n",
       " 'to',\n",
       " 'sailed',\n",
       " 'with',\n",
       " 'hope',\n",
       " 'and',\n",
       " 'illusion',\n",
       " 'tragedies',\n",
       " 'maritime',\n",
       " 'atlantic',\n",
       " 'sinking',\n",
       " 'was',\n",
       " 'one',\n",
       " 'greatest',\n",
       " 'cross']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 0.7191647887229919),\n",
       " ('was', 0.5374302268028259),\n",
       " ('titanic', 0.3582616448402405)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los tres m치s parecidos a esa palabra\n",
    "\n",
    "model.wv.most_similar(\"maritime\",topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('titanic', 0.5306490063667297),\n",
       " ('tragedies', 0.3752540647983551),\n",
       " ('one', 0.3642173409461975)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Las m치s parecidas a the y maritime sin incluir and en el conteo\n",
    "model.wv.most_similar(positive=[\"the\",\"maritime\"],negative=[\"and\"],topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7727770805358887"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distancia entre dos palabras, cuanto menor, menor distancia, realiza el producto excalar de dos vectores\n",
    "model.wv.distance(\"the\",\"maritime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = sum(model.wv[w] for w in token1) / len (token1)\n",
    "embedding2 = sum(model.wv[w] for w in token2) / len (token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5285577331217381\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "# Da la distancia del coseno para calcular con los dos arrays de los tokens \n",
    "\n",
    "cosine_dist =  distance.cosine(embedding1,embedding2)\n",
    "print(cosine_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
