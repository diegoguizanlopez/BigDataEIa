{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mediapipe-model-maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego.guizanlopez\\AppData\\Local\\miniconda3\\envs\\ia\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\diego.guizanlopez\\AppData\\Local\\miniconda3\\envs\\ia\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego.guizanlopez\\AppData\\Local\\miniconda3\\envs\\ia\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "from mediapipe_model_maker import gesture_recognizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=str(pathlib.Path().resolve())+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genericpath import isfile\n",
    "from os import listdir, mkdir\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(path+\"Piedra_Papel_Tijera_Formated\"):\n",
    "    gestos=([],[],[],[])\n",
    "    directories = [f for f in listdir(path+\"Piedra_Papel_Tijeras\")]\n",
    "    for i in directories:\n",
    "        for index,(dirpath, dirnames, filenames) in enumerate(os.walk(path+\"Piedra_Papel_Tijeras/\"+i+\"/\")):\n",
    "            if len(dirnames)==0:\n",
    "                for filename in filenames:\n",
    "                    gestos[index-1].append(dirpath+\"/\"+filename)\n",
    "    mkdir(path+\"Piedra_Papel_Tijera_Formated\")\n",
    "    mkdir(path+\"Piedra_Papel_Tijera_Formated/None\")\n",
    "    mkdir(path+\"Piedra_Papel_Tijera_Formated/Piedra\")\n",
    "    mkdir(path+\"Piedra_Papel_Tijera_Formated/Papel\")\n",
    "    mkdir(path+\"Piedra_Papel_Tijera_Formated/Tijeras\")\n",
    "\n",
    "# Comprueba si el directorio existe\n",
    "    if not os.path.exists(path):\n",
    "    # Si no existe, crÃ©alo\n",
    "        os.makedirs(path)\n",
    "    for index,i in enumerate(gestos):\n",
    "        for j in i:\n",
    "            split=i[index].split(\"/\")\n",
    "            split.remove(split[-3])\n",
    "            shutil.copy(j,path+\"Piedra_Papel_Tijera_Formated/\"+split[-2].title())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "File loading is not yet supported on Windows",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mgesture_recognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPiedra_Papel_Tijera_Formated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgesture_recognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHandDataPreprocessingParams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m train_data, rest_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m      6\u001b[0m validation_data, test_data \u001b[38;5;241m=\u001b[39m rest_data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\diego.guizanlopez\\AppData\\Local\\miniconda3\\envs\\ia\\lib\\site-packages\\mediapipe_model_maker\\python\\vision\\gesture_recognizer\\dataset.py:202\u001b[0m, in \u001b[0;36mDataset.from_folder\u001b[1;34m(cls, dirname, hparams)\u001b[0m\n\u001b[0;32m    195\u001b[0m all_gesture_indices \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    196\u001b[0m     index_by_label[os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(path))]\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m all_image_paths\n\u001b[0;32m    198\u001b[0m ]\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# Compute hand data (including local hand landmark, world hand landmark, and\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# handedness) for all the input images.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m hand_data \u001b[38;5;241m=\u001b[39m \u001b[43m_get_hand_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_image_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_image_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Get a list of the valid hand landmark sample in the hand data list.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m valid_indices \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    208\u001b[0m     i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hand_data)) \u001b[38;5;28;01mif\u001b[39;00m hand_data[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    209\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\diego.guizanlopez\\AppData\\Local\\miniconda3\\envs\\ia\\lib\\site-packages\\mediapipe_model_maker\\python\\vision\\gesture_recognizer\\dataset.py:114\u001b[0m, in \u001b[0;36m_get_hand_data\u001b[1;34m(all_image_paths, min_detection_confidence)\u001b[0m\n\u001b[0;32m    104\u001b[0m hand_landmarker_writer \u001b[38;5;241m=\u001b[39m metadata_writer\u001b[38;5;241m.\u001b[39mHandLandmarkerMetadataWriter(\n\u001b[0;32m    105\u001b[0m     hand_detector_model_buffer, hand_landmarks_detector_model_buffer)\n\u001b[0;32m    106\u001b[0m hand_landmarker_options \u001b[38;5;241m=\u001b[39m _HandLandmarkerOptions(\n\u001b[0;32m    107\u001b[0m     base_options\u001b[38;5;241m=\u001b[39mbase_options_module\u001b[38;5;241m.\u001b[39mBaseOptions(\n\u001b[0;32m    108\u001b[0m         model_asset_buffer\u001b[38;5;241m=\u001b[39mhand_landmarker_writer\u001b[38;5;241m.\u001b[39mpopulate()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    113\u001b[0m )\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_HandLandmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhand_landmarker_options\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m hand_landmarker:\n\u001b[0;32m    116\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m all_image_paths:\n\u001b[0;32m    117\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading image \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, path)\n",
      "File \u001b[1;32mc:\\Users\\diego.guizanlopez\\AppData\\Local\\miniconda3\\envs\\ia\\lib\\site-packages\\mediapipe\\tasks\\python\\vision\\hand_landmarker.py:271\u001b[0m, in \u001b[0;36mHandLandmarker.create_from_options\u001b[1;34m(cls, options)\u001b[0m\n\u001b[0;32m    254\u001b[0m   options\u001b[38;5;241m.\u001b[39mresult_callback(hand_landmarks_detection_result, image,\n\u001b[0;32m    255\u001b[0m                           timestamp\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m _MICRO_SECONDS_PER_MILLISECOND)\n\u001b[0;32m    257\u001b[0m task_info \u001b[38;5;241m=\u001b[39m _TaskInfo(\n\u001b[0;32m    258\u001b[0m     task_graph\u001b[38;5;241m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[0;32m    259\u001b[0m     input_streams\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m     ],\n\u001b[0;32m    270\u001b[0m     task_options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_graph_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_flow_limiting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_RunningMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIVE_STREAM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackets_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diego.guizanlopez\\AppData\\Local\\miniconda3\\envs\\ia\\lib\\site-packages\\mediapipe\\tasks\\python\\vision\\core\\base_vision_task_api.py:65\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[1;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m packet_callback:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     64\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback should not be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runner \u001b[38;5;241m=\u001b[39m \u001b[43m_TaskRunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacket_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_mode \u001b[38;5;241m=\u001b[39m running_mode\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File loading is not yet supported on Windows"
     ]
    }
   ],
   "source": [
    "data = gesture_recognizer.Dataset.from_folder(\n",
    "    dirname=os.path.join(path,\"Piedra_Papel_Tijera_Formated\"),\n",
    "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
    ")\n",
    "train_data, rest_data = data.split(0.8)\n",
    "validation_data, test_data = rest_data.split(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'label_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m hparams \u001b[38;5;241m=\u001b[39m gesture_recognizer\u001b[38;5;241m.\u001b[39mHParams(export_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexported_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m options \u001b[38;5;241m=\u001b[39m gesture_recognizer\u001b[38;5;241m.\u001b[39mGestureRecognizerOptions(hparams\u001b[38;5;241m=\u001b[39mhparams)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgesture_recognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGestureRecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diego.guizanlopez\\AppData\\Local\\miniconda3\\envs\\ia\\lib\\site-packages\\mediapipe_model_maker\\python\\vision\\gesture_recognizer\\gesture_recognizer.py:87\u001b[0m, in \u001b[0;36mGestureRecognizer.create\u001b[1;34m(cls, train_data, validation_data, options)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options\u001b[38;5;241m.\u001b[39mhparams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m   options\u001b[38;5;241m.\u001b[39mhparams \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mHParams()\n\u001b[0;32m     86\u001b[0m gesture_recognizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m---> 87\u001b[0m     label_names\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_names\u001b[49m,\n\u001b[0;32m     88\u001b[0m     model_options\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mmodel_options,\n\u001b[0;32m     89\u001b[0m     hparams\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mhparams)\n\u001b[0;32m     91\u001b[0m gesture_recognizer\u001b[38;5;241m.\u001b[39m_create_model()\n\u001b[0;32m     93\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mgen_tf_dataset(\n\u001b[0;32m     94\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m     95\u001b[0m     is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     96\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mshuffle)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'label_names'"
     ]
    }
   ],
   "source": [
    "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
    "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
    "model = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train,\n",
    "    validation_data=validation,\n",
    "    options=options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
