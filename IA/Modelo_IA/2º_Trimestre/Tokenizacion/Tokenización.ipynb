{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"\"\"Podemos pensar al texto como una secuencia de caracteres, de palabras, de frases y\n",
    "entidades nombradas, de oraciones, párrafos, etc. Vamos a empezar por las más básicas.\n",
    "Podemos pensar en un texto como una secuencia de palabras y a la palabra como una\n",
    "secuencia de letras que juntas le dan forma y nos remiten a algun concepto. Varias palabras,\n",
    "junto con algunos símbolos como signos de puntuación, interrogación o exclamación terminan\n",
    "por otorgarle el significado. Supongamos que tenemos las siguientes secuencias\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n ,.ABCDEFGIJLMNOPQRSTUVXYZabcdefgijlmnopqrstuvxyzÁÍÓáíó'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de texto\n",
    "vocabulario = sorted(list(set(txt.lower()+txt.upper())))\n",
    "\"\".join(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, ',': 2, '.': 3, 'A': 4, 'B': 5, 'C': 6, 'D': 7, 'E': 8, 'F': 9, 'G': 10, 'I': 11, 'J': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'X': 24, 'Y': 25, 'Z': 26, 'a': 27, 'b': 28, 'c': 29, 'd': 30, 'e': 31, 'f': 32, 'g': 33, 'i': 34, 'j': 35, 'l': 36, 'm': 37, 'n': 38, 'o': 39, 'p': 40, 'q': 41, 'r': 42, 's': 43, 't': 44, 'u': 45, 'v': 46, 'x': 47, 'y': 48, 'z': 49, 'Á': 50, 'Í': 51, 'Ó': 52, 'á': 53, 'í': 54, 'ó': 55}\n",
      "{0: '\\n', 1: ' ', 2: ',', 3: '.', 4: 'A', 5: 'B', 6: 'C', 7: 'D', 8: 'E', 9: 'F', 10: 'G', 11: 'I', 12: 'J', 13: 'L', 14: 'M', 15: 'N', 16: 'O', 17: 'P', 18: 'Q', 19: 'R', 20: 'S', 21: 'T', 22: 'U', 23: 'V', 24: 'X', 25: 'Y', 26: 'Z', 27: 'a', 28: 'b', 29: 'c', 30: 'd', 31: 'e', 32: 'f', 33: 'g', 34: 'i', 35: 'j', 36: 'l', 37: 'm', 38: 'n', 39: 'o', 40: 'p', 41: 'q', 42: 'r', 43: 's', 44: 't', 45: 'u', 46: 'v', 47: 'x', 48: 'y', 49: 'z', 50: 'Á', 51: 'Í', 52: 'Ó', 53: 'á', 54: 'í', 55: 'ó'}\n"
     ]
    }
   ],
   "source": [
    "# Dos diccionarios 1 que tiene clave formato, id\n",
    "# Dos diccionarios 1 que tiene clave id, formato\n",
    "stoi = {ch:i for i,ch in enumerate(vocabulario)}\n",
    "itos = {i:ch for i,ch in enumerate(vocabulario)}\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ABC'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokeniza(value):\n",
    "    return [stoi[c] for c in value]\n",
    "\n",
    "def destokeniza(ltokens):\n",
    "    return \"\".join([itos[i] for i in ltokens])\n",
    "\n",
    "v=tokeniza(['A','B','C'])\n",
    "print(v)\n",
    "destokeniza(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Método 1 de Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¿Prueba', 'de', 'texto', 'tokenizado', 'por', 'palabras?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "test = \"¿Prueba de texto tokenizado por palabras?\"\n",
    "WhitespaceTokenizer().tokenize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Método 2 de Tokenizer, mejor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¿Prueba', 'de', 'texto', 'tokenizado', 'por', 'palabras', '?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "TreebankWordTokenizer().tokenize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Método 2 de Tokenizer, aún mejor pero no perfecto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenized=WordPunctTokenizer().tokenize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Borrar sufijos a lo burro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Prueba de texto tokenizado por palabras?\n",
      "['¿', 'prueb', 'de', 'text', 'tokeniz', 'por', 'palabr', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "print(test)\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "text = [stemmer.stem(i) for i in tokenized]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mucho mejor que los anteriores pero aún hay mejores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download(\"es\")\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma')\n",
    "texto_pablito = \"Pablito clavó un clavito cuantos clavitos clava pablito\"\n",
    "doc = nlp(texto_pablito)\n",
    "print(*[f'Palabra: {word.text+\" \"}\\tLemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora el bueno de verdad**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
